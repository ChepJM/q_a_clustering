{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans, AgglomerativeClustering, Birch, SpectralClustering, DBSCAN\n",
    "from sklearn.utils import class_weight\n",
    "from natasha import NamesExtractor, MorphVocab\n",
    "from scipy.sparse import hstack, vstack\n",
    "from collections import Counter\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "import tensorflow_hub\n",
    "import hdbscan\n",
    "import scipy.sparse\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "import os\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление мусорных символов и пунктуации\n",
    "def remove_trash(text: str) -> str:\n",
    "    pattern = '[^А-Яа-яЁё0-9 ]+'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Лемматизация\n",
    "def lemmatize(text: str) -> str:\n",
    "    t = []\n",
    "    for word in text.split():\n",
    "        if len(word)<3:\n",
    "            continue\n",
    "        p = morph.parse(word)[0]\n",
    "        t.append(p.normal_form)\n",
    "    return \" \".join(t)\n",
    "\n",
    "# Функция получения списка монограмм, биграмм и триграмм\n",
    "def get_gramms(series: pd.Series) -> list:\n",
    "    text_clean = copy.deepcopy(series.values.tolist())\n",
    "    text_clean = [sentence.split() for sentence in text_clean]\n",
    "    \n",
    "    bigramm = Phrases(text_clean) # Создаем биграммы на основе корпуса\n",
    "    trigram = Phrases(bigramm[text_clean])# Создаем триграммы на основе корпуса\n",
    "    \n",
    "    for idx in range(len(text_clean)):\n",
    "        gramms = set()\n",
    "        for token in bigramm[text_clean[idx]]:\n",
    "            if '_' in token:\n",
    "                # биграмма, добавим в документ\n",
    "                gramms.update([token])\n",
    "                break\n",
    "        for token in trigram[bigramm[text_clean[idx]]]:\n",
    "            if '_' in token:\n",
    "                # триграмма, добавим в документ\n",
    "                gramms.update([token])\n",
    "        text_clean[idx].extend(list(gramms))\n",
    "    return [gramm.replace('_', ' ') for sentence in text_clean for gramm in sentence if gramm != 'не']\n",
    "\n",
    "# Удаление 100 наиболее встречающихся слов и словосочетаний\n",
    "def remove_most_common(series: pd.Series, gramms: list) -> pd.Series:\n",
    "    stop_words = Counter(gramms).most_common(100)\n",
    "    pattern = r'\\b' + r'\\b|\\b'.join([x[0].lower() for x in stop_words]) + r'\\b'\n",
    "    return series.apply(lambda row: re.sub(pattern, '', row.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление мусора и лемматизация\n",
    "df['q_edit'] = df['question'].apply(remove_trash)\n",
    "df['q_edit'] = df['question'].apply(lemmatize)\n",
    "df['a_edit'] = df['answer'].apply(remove_trash)\n",
    "df['a_edit'] = df['answer'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление наиболее часто встречающихся слов и словосочетаний для вопросов клиентов\n",
    "gramms = get_gramms(df['q_edit'])\n",
    "df['q_edit'] = remove_most_common(df['q_edit'], gramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление наиболее часто встречающихся слов и словосочетаний для ответов консультантов\n",
    "gramms = get_gramms(df['a_edit'])\n",
    "df['a_edit'] = remove_most_common(df['a_edit'], gramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление пустых строк\n",
    "df = df[(df['a_edit'] != '') & (df['q_edit'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('average_word_embeddings_glove.6B.300d')\n",
    "# model = SentenceTransformer('average_word_embeddings_komninos')\n",
    "# model = SentenceTransformer('saverage_word_embeddings_levy_dependency')\n",
    "# model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "# model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "# model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "# model = SentenceTransformer('paraphrase-xlm-r-multilingual-v1')\n",
    "# model = SentenceTransformer('quora-distilbert-multilingual_part')\n",
    "# model = SentenceTransformer('stsb-xlm-r-multilingual_part')\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение эмбедингов для вопросов\n",
    "questions_emb = model.encode(df['q_edit'])\n",
    "# Получение эмбедингов для ответов\n",
    "answers_emb = model.encode(df['a_edit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение эмбедингов вопросов на жесткий диск\n",
    "questions_pickle = open('questions_pickle', 'wb')\n",
    "pickle.dump(questions_emb, questions_pickle)\n",
    "questions_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение эмбедингов ответов на жесткий диск\n",
    "answers_pickle = open('answers_pickle', 'wb')\n",
    "pickle.dump(answers_emb, answers_pickle)\n",
    "answers_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка эмбедингов вопросов\n",
    "questions_pickle = open('questions_pickle', 'rb')\n",
    "questions_emb = pickle.load(questions_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка эмбедингов ответов\n",
    "answers_pickle = open('answers_pickle', 'rb')\n",
    "answers_emb = pickle.load(answers_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_n_clusters = 200\n",
    "clmethod = MiniBatchKMeans(n_clusters=_n_clusters, random_state=42)\n",
    "# clmethod = KMeans(n_clusters=_n_clusters, random_state=42)\n",
    "# clmethod = AgglomerativeClustering(n_clusters=_n_clusters, linkage=\"ward\")\n",
    "# clmethod = AgglomerativeClustering(n_clusters=_n_clusters, linkage=\"average\", affinity=\"euclidean\")\n",
    "# clmethod = Birch(n_clusters=_n_clusters)\n",
    "# clmethod = SpectralClustering(n_clusters=_n_clusters, eigen_solver=None, random_state=42, affinity=\"rbf\", assign_labels=\"discretize\", n_jobs=psutil.cpu_count())\n",
    "# clmethod = DBSCAN(min_samples=10, eps=0.9)\n",
    "# clmethod = hdbscan.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True, gen_min_span_tree=False, leaf_size=40, metric='euclidean', min_cluster_size=5, min_samples=None, p=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_clids = clmethod.fit_predict(questions_emb)\n",
    "a_clids = clmethod.fit_predict(answers_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['q_cluster'] = q_clids\n",
    "df['a_cluster'] = a_clids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подсчет весов отношений ответов и кластеров вопросов\n",
    "weights = []\n",
    "for i in range(200):\n",
    "    weight = {x: 0 for x in range(200)}\n",
    "    cluster_i = df[df['q_cluster'] == i]\n",
    "    for item in tqdm(range(cluster_i.shape[0])):\n",
    "        deal_number = cluster_0.iloc[item]['deal']\n",
    "        date = cluster_0.iloc[item]['created_at']\n",
    "        try:\n",
    "            cluster_number = cons_answ[(cons_answ['deal'] == deal_number) & (cons_answ['created_at'] >= date)]['cluster'][:1].values[0]\n",
    "            weight[int(cluster_number)] += 1\n",
    "        except IndexError:\n",
    "            continue\n",
    "    weight = {x: y / cluster_i.shape[0] for x, y in weight.items() if y != 0}\n",
    "    weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим первые 20 предложений, наиболее близких к центроиде кластера 0\n",
    "distances = np.sqrt(np.sum(np.square(questions_emb - clmethod.cluster_centers_[0]), axis=1))\n",
    "indexes = [x[0] for x in sorted(enumerate(distances), key=lambda x: x[1])[:20]]\n",
    "df.iloc[indexes]['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Не подошло"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_russian = stopwords.words('russian')\n",
    "\n",
    "text_transformer = TfidfVectorizer(stop_words=stop_russian)\n",
    "text = text_transformer.fit_transform(df[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_model = tensorflow_hub.load(r\"universal-sentence-encoder-multilingual_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение эмбедингов для вопросов\n",
    "questions_emb = use_model(df['q_edit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение эмбедингов для ответов\n",
    "answer_emb = use_model(df['a_edit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cons_answ_emb = use_model(cons_answ['message'][:5000])\n",
    "# for i in range(1, cons_answ['message'].shape[0] // 5000):\n",
    "#     cons_answ_emb2 = use_model(cons_answ['message'][i*5000:(i+1)*5000])\n",
    "#     cons_answ_emb = tf.concat([cons_answ_emb, cons_answ_emb2], 0)\n",
    "# if (i+1)*5000 < cons_answ['message'].shape[0]:\n",
    "#     cons_answ_emb2 = use_model(cons_answ['message'][(i+1)*5000:])\n",
    "#     cons_answ_emb = tf.concat([cons_answ_emb, cons_answ_emb2], 0)\n",
    "# cons_answ_emb = np.array(cons_answ_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
